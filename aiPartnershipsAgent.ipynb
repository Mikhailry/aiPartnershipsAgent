{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12dc4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.my_llm_utils import *\n",
    "import pandas as pd\n",
    "\n",
    "#from utils.crawler_utils import crawl_url\n",
    "from utils.my_llm_utils import get_llm, validate_link, summarize_text_partnership, Config\n",
    "from utils.tavily_search_utils import web_search\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8984cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi4:latest\n"
     ]
    }
   ],
   "source": [
    "# load the LLM\n",
    "llm = get_llm()\n",
    "\n",
    "# Check which llm has been loaded\n",
    "if hasattr(llm, 'model'):\n",
    "\tprint(llm.model)  # check loaded model\n",
    "else:\n",
    "\tprint(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61211591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test queries\n",
    "\n",
    "# # # test query\n",
    "# # response = llm.invoke(\"What is the capital of Russia?\")\n",
    "# # print(response)\n",
    "\n",
    "# testUrl = \"https://www.crn.com/news/ai/2024/accenture-to-train-30-000-staff-on-nvidia-ai-tech-in-blockbuster-deal\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------\n",
    "# # test search_tavily\n",
    "# # ------------------------------------------------------\n",
    "# search_tavily(\"NVIDIA AI\")\n",
    "\n",
    "# #------------------------------------------------------\n",
    "# # test crawl_url\n",
    "# # ------------------------------------------------------\n",
    "# text = crawl_url(testUrl)\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------\n",
    "# # test summarize_text\n",
    "# # ------------------------------------------------------\n",
    "# summary = summarize_text(text)\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Accenture Nvidia partnership\"\n",
    "# search_response = search_tavily(query)\n",
    "# print(search_response)\n",
    "\n",
    "# text = search_response['results'][0]['content']\n",
    "\n",
    "# summary = summarize_text(text)\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ed07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(date_str):\n",
    "    \"\"\"Convert dates like 'Sep-24' to a standardized format 'yyyy-mm'\"\"\"\n",
    "    if not isinstance(date_str, str) or not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Map month abbreviations to numbers\n",
    "        month_map = {\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "            'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "            'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "        \n",
    "        # Split the date string into month and year\n",
    "        parts = date_str.strip().split('-')\n",
    "        if len(parts) != 2:\n",
    "            return None\n",
    "            \n",
    "        month_abbr, year = parts\n",
    "        \n",
    "        # Convert year to 4-digits (assuming 20xx for brevity)\n",
    "        if len(year) == 2:\n",
    "            year = f\"20{year}\"\n",
    "            \n",
    "        # Get month number\n",
    "        month_num = month_map.get(month_abbr, None)\n",
    "        if not month_num:\n",
    "            return None\n",
    "            \n",
    "        return f\"{year}-{month_num}\"\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7aba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_text(text):\n",
    "    \"\"\"Try to extract a date from text using patterns\"\"\"\n",
    "    # Look for common date patterns\n",
    "    patterns = [\n",
    "        r'(\\w+\\s+\\d{1,2},\\s*20\\d{2})',  # e.g., \"October 23, 2024\"\n",
    "        r'(\\d{1,2}\\s+\\w+\\s+20\\d{2})',    # e.g., \"23 October 2024\"\n",
    "        r'(20\\d{2}-\\d{1,2}-\\d{1,2})'     # e.g., \"2024-10-23\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            try:\n",
    "                date_str = matches[0]\n",
    "                # Try to parse the date\n",
    "                date_obj = None\n",
    "                \n",
    "                # Try different formats\n",
    "                formats = [\n",
    "                    '%B %d, %Y',      # \"October 23, 2024\"\n",
    "                    '%b %d, %Y',      # \"Oct 23, 2024\"\n",
    "                    '%d %B %Y',       # \"23 October 2024\"\n",
    "                    '%d %b %Y',       # \"23 Oct 2024\"\n",
    "                    '%Y-%m-%d'        # \"2024-10-23\"\n",
    "                ]\n",
    "                \n",
    "                for fmt in formats:\n",
    "                    try:\n",
    "                        date_obj = datetime.strptime(date_str, fmt)\n",
    "                        if date_obj:\n",
    "                            # Return in month-year format\n",
    "                            return f\"{date_obj.strftime('%b')}-{date_obj.strftime('%y')}\"\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c317e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_partnership_info(partner1, partner2, partner3=None):\n",
    "    \"\"\"Search for partnership information using Tavily\"\"\"\n",
    "    partners = [p for p in [partner1, partner2, partner3] if p and isinstance(p, str)]\n",
    "    \n",
    "    if len(partners) < 2:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Construct search query\n",
    "    query = f\"{partner1} {partner2} partnership announcement\"\n",
    "    if partner3 and isinstance(partner3, str) and partner3.strip():\n",
    "        query += f\" {partner3}\"\n",
    "        \n",
    "    print(f\"Searching for: {query}\")\n",
    "    \n",
    "    # Search for the partnership info\n",
    "    try:\n",
    "        search_results = web_search(query, max_results=3)\n",
    "        \n",
    "        if not search_results or not search_results.get('results'):\n",
    "            return None, None, None\n",
    "            \n",
    "        # Filter results to find relevant information\n",
    "        relevant_results = []\n",
    "        for result in search_results['results']:\n",
    "            # Check if both company names are in the title or content\n",
    "            title = result.get('title', '').lower()\n",
    "            content = result.get('content', '').lower()\n",
    "            full_text = title + ' ' + content\n",
    "            \n",
    "            # Check if both main partners are mentioned\n",
    "            if partner1.lower() in full_text and partner2.lower() in full_text:\n",
    "                # Check for words indicating partnership\n",
    "                partnership_keywords = ['partner', 'collaboration', 'agreement', 'alliance', 'join']\n",
    "                if any(keyword in full_text for keyword in partnership_keywords):\n",
    "                    relevant_results.append(result)\n",
    "        \n",
    "        # Use the most relevant result or fall back to the first one\n",
    "        result = relevant_results[0] if relevant_results else search_results['results'][0]\n",
    "        \n",
    "        link = result.get('url')\n",
    "        content = result.get('content', '')\n",
    "        \n",
    "        # Try to extract date from content\n",
    "        date = extract_date_from_text(content)\n",
    "        \n",
    "        # Generate summary if we have content\n",
    "        summary = None\n",
    "        if content and len(content.strip()) > 10:\n",
    "            summary = summarize_text_partnership(content, partner1, partner2)\n",
    "            \n",
    "        return link, date, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching for partnership info: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a20cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_partnerships(csv_path, output_path=None):\n",
    "    \"\"\"Update the AI partnerships CSV with missing information\"\"\"\n",
    "    if not output_path:\n",
    "        # Create output filename based on input filename\n",
    "        dirname, filename = os.path.split(csv_path)\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        output_path = os.path.join(dirname, f\"{base}_updated{ext}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Add summary column if it doesn't exist\n",
    "    if 'summary' not in df.columns:\n",
    "        df['summary'] = None\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        partner1 = row.get('partner1')\n",
    "        partner2 = row.get('partner2')\n",
    "        partner3 = row.get('partner3')\n",
    "        \n",
    "        # Skip if we don't have at least two partners\n",
    "        if not isinstance(partner1, str) or not isinstance(partner2, str):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing partnership: {partner1} and {partner2}\")\n",
    "        \n",
    "        # Check if we have a valid link\n",
    "        link1 = row.get('Link')\n",
    "        link2 = row.get('link 2')\n",
    "        \n",
    "        valid_link = None\n",
    "        content = None\n",
    "        \n",
    "        # Validate existing links\n",
    "        if isinstance(link1, str) and validate_link(link1):\n",
    "            valid_link = link1\n",
    "        elif isinstance(link2, str) and validate_link(link2):\n",
    "            valid_link = link2\n",
    "        \n",
    "        # If no valid link or missing date, search for information\n",
    "        if not valid_link or pd.isna(row.get('When announced')):\n",
    "            new_link, new_date, summary = find_partnership_info(partner1, partner2, partner3)\n",
    "            \n",
    "            # Update link if we found one\n",
    "            if new_link and validate_link(new_link):\n",
    "                df.at[idx, 'Link'] = new_link\n",
    "                valid_link = new_link\n",
    "                print(f\"Found new link: {new_link}\")\n",
    "                \n",
    "            # Update date if we found one\n",
    "            if new_date and (pd.isna(row.get('When announced')) or not row.get('When announced')):\n",
    "                df.at[idx, 'When announced'] = new_date\n",
    "                print(f\"Found new date: {new_date}\")\n",
    "                \n",
    "            # Update summary if we found one\n",
    "            if summary:\n",
    "                df.at[idx, 'summary'] = summary\n",
    "                print(f\"Generated summary: {summary}\")\n",
    "            \n",
    "        # If we have a valid link but no summary, generate one\n",
    "        elif valid_link and (pd.isna(row.get('summary')) or not row.get('summary')):\n",
    "            # Here we would fetch the content from the link, but for simplicity we'll\n",
    "            # just use the link itself for the search\n",
    "            print(f\"Generating summary based on existing link...\")\n",
    "            search_results = web_search(valid_link, max_results=1)\n",
    "            \n",
    "            if search_results and search_results.get('results'):\n",
    "                content = search_results['results'][0].get('content', '')\n",
    "                if content and len(content.strip()) > 10:\n",
    "                    summary = summarize_text_partnership(content, partner1, partner2)\n",
    "                    df.at[idx, 'summary'] = summary\n",
    "                    print(f\"Generated summary: {summary}\")\n",
    "                    \n",
    "        # Add a small delay to avoid rate limits\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # Save the updated data\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nUpdated data saved to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ded667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for the input file path\n",
    "default_path = \"data/AI Partnerships sample.csv\"\n",
    "    \n",
    "# Check if the default file exists\n",
    "if not os.path.exists(default_path):\n",
    "    csv_path = input(f\"Enter the path to the AI partnerships CSV file (default: {default_path}): \")\n",
    "    if not csv_path:\n",
    "        csv_path = default_path\n",
    "else:\n",
    "    csv_path = default_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2326e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing partnership: Hugging Face and AI Alliance\n",
      "Generating summary based on existing link...\n",
      "Generated summary: IBM and Meta have launched a new AI Alliance, collaborating with other tech leaders to promote the advancement of open, safe, and responsible artificial intelligence.\n",
      "\n",
      "\n",
      "\n",
      "Processing partnership: Meta and AI Alliance\n",
      "Generating summary based on existing link...\n",
      "Generated summary: IBM and Meta have launched an AI Alliance, partnering with other tech leaders, to promote the development of open, safe, and responsible artificial intelligence.\n",
      "\n",
      "**Note:** While I strive to provide accurate summaries based on given information, please verify details from official announcements for comprehensive understanding.\n",
      "\n",
      "Processing partnership: Anthropic and Democracy works\n",
      "Searching for: Anthropic Democracy works partnership announcement\n",
      "Found new link: https://democracy.works/news/democracy-works-partnering-with-anthropic\n",
      "Generated summary: Democracy Works is partnering with Anthropic to direct users toward authoritative election information through generative AI, aiming to prevent AI misuse in political contexts while enhancing voter support and planning to expand this initiative internationally based on insights from their TurboVote redirect.\n",
      "\n",
      "This partnership highlights Democracy Works' commitment to providing reliable voting guidance and Anthropic's focus on building a responsible AI platform prioritizing user trust and safety.\n",
      "\n",
      "Processing partnership: Salesforce and AWS\n",
      "Generating summary based on existing link...\n",
      "\n",
      "Processing partnership: Salesforce and Google\n",
      "Generating summary based on existing link...\n",
      "\n",
      "Processing partnership: Hugging Face and IBM\n",
      "Searching for: Hugging Face IBM partnership announcement\n",
      "Found new link: https://newsroom.ibm.com/2023-08-24-IBM-to-Participate-in-235M-Series-D-Funding-Round-of-Hugging-Face\n",
      "Generated summary: IBM has joined Hugging Face’s $235M series D funding round, strengthening their collaboration on WatsonX to facilitate enterprise-level deployment of NLP models. This partnership includes IBM contributing over 200 open models to Hugging Face, such as the largest geospatial foundation model developed with NASA, and plans to host Meta's Llama 2-chat model within WatsonX, showcasing a strategy that combines third-party and proprietary AI resources.\n",
      "\n",
      "This summary captures the key elements of the announcement: the financial investment by IBM in Hugging Face, their collaborative efforts on IBM's generative AI platform WatsonX, the shared models and datasets including a notable collaboration with NASA, and IBM's strategy involving Meta’s model.\n",
      "\n",
      "Processing partnership: IBM and Hugging Face\n",
      "Searching for: IBM Hugging Face partnership announcement\n",
      "Found new link: https://newsroom.ibm.com/2023-08-24-IBM-to-Participate-in-235M-Series-D-Funding-Round-of-Hugging-Face\n",
      "Generated summary: IBM has joined a $235 million Series D funding round for Hugging Face and is collaborating with them on IBM's generative AI platform, WatsonX, to assist enterprises in building, deploying, and customizing foundation models. This partnership includes leveraging over 200 open models from IBM and hosting Meta's Llama 2-chat model within WatsonX, bolstering Hugging Face's position as a leading open-source AI platform.\n",
      "\n",
      "---\n",
      "\n",
      "This summary captures the key elements of the announcement: financial investment, collaborative efforts on AI platforms, contributions of models, and strategic objectives to enhance enterprise capabilities in AI.\n",
      "\n",
      "Processing partnership: Salesforce and Cohere\n",
      "Generating summary based on existing link...\n",
      "Generated summary: Salesforce has announced a partnership with Cohere to integrate advanced AI capabilities into its products, enhancing user workflows and offering more sophisticated AI tools.\n",
      "\n",
      "(Note: The summary is based on the context provided; however, specific details about this particular announcement were not included in your input. For precise information, reviewing the full announcement would be necessary.)\n",
      "\n",
      "Processing partnership: NIVIDIA  and Waabi\n",
      "Generating summary based on existing link...\n",
      "Generated summary: On March 18, 2024, NVIDIA announced a partnership with Waabi to develop the first generative AI-powered autonomous trucking solution, integrating NVIDIA's advanced DRIVE Thor compute platform into Waabi's technology for enhanced performance and scalability in self-driving applications.\n",
      "\n",
      "\n",
      "\n",
      "Processing partnership: NVIDIA and Tata Communications\n",
      "Generating summary based on existing link...\n",
      "Generated summary: NVIDIA has deepened its AI initiatives in India by forming new partnerships, including one with Tata Communications, as part of a broader strategy to expand its presence and capabilities in the region.\n",
      "\n",
      "\n",
      "\n",
      "Updated data saved to data/AI Partnerships sample_updated.csv\n"
     ]
    }
   ],
   "source": [
    " # Update the partnerships data\n",
    "success = update_partnerships(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
